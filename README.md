# AnyModeAssistant
AI assistant using Phi2 model

## Training the model from scratch

- Adopting the config of phi2 model, new model is created with random weights
- Training data: slim-pajama dataset and custome dataset (using trained phi-2 model)
- loss was reduced from 8 to 4.

## Any-mode Assistant

- CLIP model is used to encode the given image into embeddings. More than 100K images are pre-embedded into 512D for the next step using the pretrained CLIP model.
- LLAVA based approached used for converting the vision embedding into language model space. This contains two step training process.

    * Training the projection head (from vision embedding space to language model embedding space) using the captions of the images from COCO 2017 dataset. Four LM tokens are generated by the projection head from the clip model output.
    * Instruction fine tuning both projection head and the Phi-2 (using LORA adaptors- PEFT) using the instruction -150K dataset.
- Wispher model is used for the transcription from audio to text.


## Possible improvements

- Currently the stage 2 training was done using conversation (question and answer pairs) only from the Fine tuning dataset. The detailed description and complex reasoning dataset can be used to fine tune the model better.
- Instead of pooled embedding, the last hidden state output could be used for projection head
  
