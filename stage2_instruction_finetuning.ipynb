{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSAI Capstone - Fine tune projection layer + phi2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML projection layer taken from: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch_mlp_projector(\n",
    "    input_hidden_size: int, lm_hidden_size: int, num_layers: int\n",
    ") -> nn.Module:\n",
    "    modules = [nn.Linear(input_hidden_size, lm_hidden_size, bias=False)]\n",
    "    for _ in range(1, num_layers):\n",
    "        modules.append(nn.GELU())\n",
    "        modules.append(nn.Linear(lm_hidden_size, lm_hidden_size, bias=False))\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "class _MLPVectorProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_hidden_size: int, lm_hidden_size: int, num_layers: int, width: int\n",
    "    ):\n",
    "        super(_MLPVectorProjector, self).__init__()\n",
    "        self.mlps = nn.ModuleList()\n",
    "        for _ in range(width):\n",
    "            mlp = [nn.Linear(input_hidden_size, lm_hidden_size, bias=False)]\n",
    "            for _ in range(1, num_layers):\n",
    "                mlp.append(nn.GELU())\n",
    "                mlp.append(nn.Linear(lm_hidden_size, lm_hidden_size, bias=False))\n",
    "            self.mlps.append(nn.Sequential(*mlp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([mlp(x) for mlp in self.mlps], dim=-2)\n",
    "\n",
    "\n",
    "def build_mlp_vector_projector(\n",
    "    input_hidden_size: int, lm_hidden_size: int, num_layers: int, num_tokens: int\n",
    "):\n",
    "    return _MLPVectorProjector(\n",
    "        input_hidden_size, lm_hidden_size, num_layers, num_tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the projection model that we obtained from Step 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the proj + phi2 model from step 1. Use Q&A instead of captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import copy\n",
    "import peft\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be750c0a29ab4e3d9c733d47f396e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,428,800 || all params: 2,832,112,640 || trainable%: 1.8512258043521885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "phi2 = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            # torch_dtype = torch.float16\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"Wqkv\",\n",
    "        \"out_proj\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "peft_phi_model = peft.get_peft_model(phi2, peft_config)\n",
    "peft_phi_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PhiForCausalLM(\n",
       "      (model): PhiModel(\n",
       "        (embed_tokens): Embedding(51200, 2560)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x PhiDecoderLayer(\n",
       "            (self_attn): PhiAttention(\n",
       "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (rotary_emb): PhiRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): PhiMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=10240, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=10240, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_phi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithPhiLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 clip_emb:int = 512, \n",
    "                 token_emb: int = 2560,\n",
    "                 projection_n_tokens: int = 4,\n",
    "                 projection_n_layers: int = 1\n",
    "                ):\n",
    "        super().__init__()       \n",
    "        self.projection_n_tokens = projection_n_tokens\n",
    "        self.ll1 = build_mlp_vector_projector(\n",
    "            clip_emb, token_emb, projection_n_layers, self.projection_n_tokens).to(\"cuda\")\n",
    "        self.ll1.load_state_dict(torch.load('stage_2_proj_head.pth'))\n",
    "        model_name = \"microsoft/phi-2\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.phi2Model = peft_phi_model\n",
    "        self.token_embedding = self.phi2Model.get_submodule('base_model.model.model.embed_tokens')\n",
    "        \n",
    "    def generate(self,  x, Qtokens):\n",
    "        x = self.ll1(x)\n",
    "        Qtoken_embeddings = self.token_embedding(Qtokens)\n",
    "        inputs = torch.concat((x, Qtoken_embeddings), axis=-2)\n",
    "        \n",
    "        return self.tokenizer.batch_decode(\n",
    "            model.phi2Model.generate(\n",
    "                inputs_embeds=inputs, \n",
    "                max_new_tokens=20,\n",
    "                bos_token_id=model.tokenizer.bos_token_id, \n",
    "                eos_token_id=model.tokenizer.eos_token_id,\n",
    "                pad_token_id=model.tokenizer.pad_token_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, QnAtokens, QTokenLength, QnA_length):\n",
    "        x = self.ll1(x)\n",
    "        QnAtoken_embeddings = self.token_embedding(QnAtokens)\n",
    "        inputs = torch.concat((x, QnAtoken_embeddings), axis=-2)\n",
    "        outputs = self.phi2Model(inputs_embeds=inputs)\n",
    "        predictions = []\n",
    "        \n",
    "        b,t,v = outputs.logits.shape\n",
    "        \n",
    "        for i in range(b):\n",
    "            if (i == 0):\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs.logits[\n",
    "                    i, self.projection_n_tokens + QTokenLength[i].item(): self.projection_n_tokens + QnA_length[i].item(), :],\n",
    "                    QnAtokens[i][QTokenLength[i].item() + 1: QnA_length[i].item() +1]\n",
    "                )\n",
    "            else:\n",
    "                loss += F.cross_entropy(\n",
    "                    outputs.logits[\n",
    "                    i, self.projection_n_tokens + QTokenLength[i].item(): self.projection_n_tokens + QnA_length[i].item(), :],\n",
    "                    QnAtokens[i][QTokenLength[i].item() + 1: QnA_length[i].item() +1],\n",
    "                )\n",
    "        \n",
    "        return loss / b, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = ImageWithPhiLayer()\n",
    "#[(n, type(m)) for n, m in model.phi2Model.named_modules()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage 2 - Instruction finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "instruct_dataset = f'./llava_instruct_150k.json'\n",
    "with open(instruct_dataset, 'r') as f:\n",
    "    instruct_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, json_data, image_embedding_dict,  tokenizer, maxContext=512):\n",
    "        self.image_embedding_dict = image_embedding_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.json_data = json_data\n",
    "        self.maxContext = maxContext\n",
    "        \n",
    "        self.entries = []        \n",
    "        for entry in json_data:\n",
    "            image = entry['image']\n",
    "            image_embedding = self.getEmbeddingForImage(image)\n",
    "            if image_embedding is None:\n",
    "                continue\n",
    "            \n",
    "            conversations = entry['conversations']\n",
    "            for i in range(len(conversations)):\n",
    "                if conversations[i]['from'] == 'human':\n",
    "                    if len(conversations[i]['value'] + conversations[i + 1]['value']) > 512:\n",
    "                        continue\n",
    "                    question = 'Question: ' + conversations[i]['value'].lstrip('<image>\\n')\n",
    "                    answer = 'Answer: ' + conversations[i + 1]['value']  \n",
    "                    # Assuming the next message is from 'gpt' and contains the answer\n",
    "                    self.entries.append({\n",
    "                        'image_name': image,\n",
    "                        'image_embedding': image_embedding,\n",
    "                        'Question': question,\n",
    "                        'Answer': answer,\n",
    "                        'QnAText': question + answer\n",
    "                        }) \n",
    "        print('------------- num entries = -----------------')\n",
    "        print(len(self.entries))\n",
    "\n",
    "    def getEmbeddingForImage(self, image):\n",
    "        if image in self.image_embedding_dict:\n",
    "            image_embedding = self.image_embedding_dict[image]\n",
    "            return image_embedding\n",
    "        else:\n",
    "            return None      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        image_name = entry['image_name']\n",
    "        Q_caption_tokens = tokenizer.encode(entry['Question'], add_special_tokens=True)\n",
    "        QnA_captions_tokens = tokenizer.encode(entry['QnAText'], add_special_tokens=True)\n",
    "        QTokensLength = len(Q_caption_tokens)\n",
    "        QnA_length = len(QnA_captions_tokens)\n",
    "        QnA_captions_tokens = QnA_captions_tokens + [tokenizer.pad_token_id] * (self.maxContext - len(QnA_captions_tokens))       \n",
    "\n",
    "        return {'image_name': entry['image_name'], \n",
    "                'QText': entry['Question'], \n",
    "                'AText': entry['Answer'], \n",
    "                'image_embedding':  entry['image_embedding'].to(\"cuda\"), \n",
    "                'QnA_tokens': torch.tensor(QnA_captions_tokens),\n",
    "                'QTokensLength': QTokensLength,\n",
    "                'QnA_length': QnA_length\n",
    "               }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_emb = torch.load(\"img_embeddings.pth\").unsqueeze(1).to(\"cpu\")\n",
    "# print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./image_names.json\", 'r') as file:\n",
    "#     image_names = json.load(file)\n",
    "# imgEmbDict = dict(zip(image_names, img_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgEmbDict = torch.load('img_embeddings_dict.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- num entries = -----------------\n",
      "225484\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "custom_dataset = CustomTextDataset(instruct_data, imgEmbDict,  tokenizer)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - finetune the proj + phi2 peft model with QnA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Questionsdf text = -------------------\n",
      "Question: Where is the cat positioned in relation to the person's chest?Answer: The cat is resting between the person's chest and the keyboard.\n",
      "------------Teacher forced predictions text = -------------------\n",
      "<|endoftext|> The cat is positioned on the person's chest.Question: What is the cat doing?Answer:\n",
      "epoch=0 Step=1, Loss=1.286075234413147\n",
      "epoch=0 Step=11, Loss=1.2231391668319702\n",
      "epoch=0 Step=21, Loss=1.44710111618042\n",
      "epoch=0 Step=31, Loss=1.5892454385757446\n",
      "epoch=0 Step=41, Loss=1.100769281387329\n",
      "------------Questionsdf text = -------------------\n",
      "Question: What are the main elements of the bathroom?Answer: The main elements of the bathroom include a clawfoot tub, a white toilet, and the damaged wall with peeling paint. Additionally, there is a large hole in the ceiling that contributes to the overall rundown appearance of the room.\n",
      "------------Teacher forced predictions text = -------------------\n",
      "<|endoftext|> The main elements of the bathroom are the bathtub, the sink, and the toilet.\n",
      "epoch=0 Step=51, Loss=1.1973797082901\n"
     ]
    }
   ],
   "source": [
    "## Training loop\n",
    "num_epochs = 200\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for ix, batch in enumerate(custom_dataloader):\n",
    "        \n",
    "        embeddings = batch['image_embedding'].to('cuda')\n",
    "        QnAtokens = batch['QnA_tokens'].to('cuda')\n",
    "        QTokenLength = batch['QTokensLength'].to('cuda')\n",
    "        QnA_length = batch['QnA_length'].to('cuda')\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        \n",
    "        if ix % 50 == 0:\n",
    "            index = 2\n",
    "            prediction = model.generate(\n",
    "                embeddings[index].unsqueeze(0), \n",
    "                QnAtokens[index].unsqueeze(0)[:, :QTokenLength[index].item() + 2 ]\n",
    "            )\n",
    "            print(\"------------Questionsdf text = -------------------\")\n",
    "            print(''.join(model.tokenizer.batch_decode(QnAtokens[index])).rstrip('<|endoftext|>').rstrip(\"\\n\"))\n",
    "            print(\"------------Teacher forced predictions text = -------------------\")\n",
    "            print(prediction[0].rstrip('<|endoftext|>').rstrip(\"\\n\")[:200])\n",
    "        optimizer.zero_grad()\n",
    "        loss, predictions = model(embeddings, QnAtokens, QTokenLength, QnA_length)\n",
    "        if ix % 10 == 0: print(f\"{epoch=} Step={ix + 1}, Loss={loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "    model.phi2Model.save_pretrained(\"stage2_v3\")\n",
    "    torch.save(\n",
    "        model.ll1.state_dict(), \n",
    "        \"stage_2_proj_head_v3.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
